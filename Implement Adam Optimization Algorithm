import numpy as np

def adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):
    m = 0
    v = 0
    x = x0

    for t in range(1, num_iterations + 1):
        """
        Note:
        The loop starts from `t = 1` (not 0) to avoid division by zero during 'bias correction' (`1 - beta1^t`, `1 - beta2^t`) and runs for `num_iterations` steps.
        """
        g = grad(x)

        # momentum(m) -> Momentum
        m = beta1 * m + (1 - beta1) * g
        
        # velocity(v) -> RMSProp
        v = beta2 * v + (1 - beta2) * (g ** 2)

        # bias correction
        m_corrected = m / (1 - beta1 ** t)
        v_corrected = v / (1 - beta2 ** t)

        # update step
        x = x - learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)

    return x


