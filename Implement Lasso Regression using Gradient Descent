import numpy as np

def l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:
	n_samples, n_features = X.shape

	weights = np.zeros(n_features)
	bias = 0

    for i in range(max_iter):
        y_hat = np.dot(X, weights) + bias
        error = y_hat - y

        # Gradient with L1 (subgradient: sign(weights))
        weights_grad = (np.dot(X.T , error) / n_samples) + alpha * np.sign(weights) 
        bias_grad = np.sum(error) / n_samples

        # update
        weights -= learning_rate * weights_grad
        bias -= learning_rate * bias_grad

        # convergence check
        if np.linalg.norm(weights_grad, ord=1) < tol:
            break

    return (weights, bias)
